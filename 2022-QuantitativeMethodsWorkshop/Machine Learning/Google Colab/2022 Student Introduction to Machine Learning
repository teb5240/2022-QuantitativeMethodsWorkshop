{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2022 Student Introduction to Machine Learning","provenance":[{"file_id":"1f0Yn3ctWYgK2Gwc4zCyuCekRYPjQpaWb","timestamp":1641441881228}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"2FH70a-o3tfk"},"source":["# Introduction to Machine Learning\n","This tutorial was composed by Taylor Baum with adaptations from the National Science Foundation (NSF) Center for Brains, Minds and Machines Quantitative Methods Workshop and [2]."]},{"cell_type":"markdown","source":["## Introduction\n","\n","An **algorithm** is similar to a recipe. More formally, an **algorithm** is a process or set of rules with which a task is completed or with which an input is manipulated to produce an output. For example, we use algorithms for tasks like sorting. \n","\n","What if there is a task where the algorithm isn’t obvious? What can we do? Well, we often have a large amount of data that we can use to help inform how we produce appropriate outputs from these inputs. As beautifully defined in [1], \"Machine Learning provides automated methods of data analysis that can then be used further. Machine Learning is a set of methods that can automatically detect patterns in data, and then use the uncovered patterns to predict future data, or to perform other kinds of decision making under uncertainty.\""],"metadata":{"id":"mzVx3wntcJT8"}},{"cell_type":"markdown","source":["## Where did the idea for Machine Learning come from?\n","\n","Traditionally, humans learn how to do certain tasks from experience in the real-world while computers must be instructed to do things. Machine learning is a field which seeks to enable computers and machines to learn from experience. The experience that computers have access to is known as data. \n","\n","Thus far, there are three different ways that a machine can learn: **supervised learning**, **unsupervised learning**, and **reinforcement learning**. We will focus on the first two methods.\n","\n","**Supervised learning** uses **labeled** data as it’s feature set or the data we initially have to learn patterns from. The labeled data is a set of input-output pairs. In other words, you have input variables (X) and their corresponding output variables (Y) and you want to learn the mapping between these variables.\n","\n","**Unsupervised learning** uses **unlabeled** data as it’s feature set or the data we initially have to learn patterns from. In **unsupervised learning**, we only have a set of inputs, and the goal is to discover interesting patterns in the data.\n","\n","Some examples of achievements which are direct results of the benefits of machine learning are AlphaGo, a system which can outperform a champion Go player, face detection on the iPhone, and various object detection paradigms. In the past three examples, we see that these tasks are complex, like a game with many rules. Before Machine Learning approaches, progress towards achieving these tasks was minimal."],"metadata":{"id":"zoniyEP9cJbf"}},{"cell_type":"markdown","source":["## How do we learn from data?\n","\n","To get an understanding of how patterns and relationships may be uncovered in data to then be used for future predictions or decision making, let's dive into some interactive examples.\n","\n","Each case where we want to use machine learning is unique, but can be boiled down into a few main components:\n","- Identifying the Input-Output Relationship to Learn\n","- Feature Extraction and Engineering\n","  - Download the Data\n","  - Prepare Input Data\n","  - Prepare Output Data\n","- Model Training\n","  - Split Data into Training and Testing Sets\n","  - Training the Model\n","- Model Validation\n","  - Assessing Accuracy of the Model\n","  - Assessig Generalizability of a Model\n","\n","We will learn that these general steps are useful to guide the development of many machine learning models. We can expand these steps to more specifically reference certain problem spaces, however, these underlying steps will still be present.\n","\n","First, we will look at an example of **supervised learning**, where we have a set of input/output pairs and we would like to find a relationship between these inputs and outputs. We will use a very simple synthetic data set."],"metadata":{"id":"v3_dYhKscU9R"}},{"cell_type":"markdown","source":["## Tutorial 1: Linear Least Squares Regression\n","\n","In this first tutorial, we will be moving through training of a linear regression. We will follow the following steps:\n","1. Import and download all necessary packages and classes\n","2. Identify and prepare the data you desire to learn about\n","3. Create the model and fit it with the existing data.\n","4. Check the results of model fitting to know whether the model is satisfactory.\n","5. Apply the model for predictions.\n","\n","We are working with dummy data that we generated, so there isn't any meaning to what we find from this exercise! But it helps us to move through the motions of training a machine learning model!"],"metadata":{"id":"nVzbB2ZZcWtS"}},{"cell_type":"markdown","metadata":{"id":"iloUoj-452cb"},"source":["### Step 1: Import Packages and Classes\n","First, we make sure to import the packages and classes necessary to run the tutorial!\n"]},{"cell_type":"code","metadata":{"id":"k1GUGrjw0fWT"},"source":["import numpy as np # a package for manipulating numbers and using arrays\n","from sklearn.linear_model import LinearRegression # a package with an optimized LinearRegression class\n","import matplotlib.pyplot as plt # a package with plotting capabilities"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9LETHt0M6Vhb"},"source":["### Step 2: Provide Data\n","\n","Next we load and prepare our data. In this case, we are using synthetic data, so there is not much we need to do!"]},{"cell_type":"code","metadata":{"id":"WGmI3Rg46Qve"},"source":["x = np.array([5, 15, 25, 35, 45, 55]).reshape((-1, 1)) # synthetic x-data\n","y = np.array([5, 20, 14, 32, 22, 38]) # synthetic y-data\n","\n","# Use these commands to look at the arrays after the .reshape function\n","print(x)\n","print(y)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HEekXn1w6bpF"},"source":["### Step 3: Create and Fit the Model\n","\n","Next, we take advantage of the `LinearRegression()` class that we downloaded from the `sklearn` package. First, we create an instance of the `LinearRegression()` class. Next, we use the `.fit(x,y)` method from this class."]},{"cell_type":"code","metadata":{"id":"6fRhWqZo6aMV"},"source":["model = LinearRegression() # this line is creating an instance of the class LinearRegression\n","model.fit([REPLACE ME AND MY BRACKETS], [REPLACE ME AND MY BRACKETS]) # this line is using a method within the LinearRegression class which fits the model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"C3u60Uw_6gWu"},"source":["We can actually condense these first two lines to one line, and get an equivalent output! These types of shortcuts make more sense the more one attemtps their own projects."]},{"cell_type":"code","metadata":{"id":"jCifeK2x6img"},"source":["model = LinearRegression().fit([REPLACE ME AND MY BRACKETS], [REPLACE ME AND MY BRACKETS]) # syntactically, this line is equivalent to the two lines above"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_l6fQgAE6sFh"},"source":["### Step 4: Get Results\n","\n","After fitting the model, we would like to explore the results and performance of the model! There are many values we can derive which describe characteristics of our fit. \n","\n","$r$, a value between -1 and 1, is the **correlation coefficient**, which will be defined in the tutorial. Values further from 0 indicate a stronger relationship between the two variables.\n","\n","To derive $r$, we use the function below. Sometimes this function is already coded in packages. We will see that the `.score(x, y)` function outputs the $r^2$ value.\n","\n","$r = \\frac{1}{n-1}\\sum\\limits_{i}^{n}{\\frac{x_i-\\bar{x}}{s_x} \\frac{y_i-\\bar{y}}{s_y}}$\n","\n","where $\\bar{x}$, $\\bar{y}$, $s_x$ and $s_y$ are the respecive means and standard deviations of the respective variables. \n","\n","$r^2$, ranging from 0 to 1, is the **coefficient of determination**. This is equal to the proportion of the total variability explained by the model. An $r^2$ of 0 means that the dependent variable cannot be predicted from the independent variable. An $r^2$ of 1 means the dependent variable can be predicted without error from the independent variable. To get a deeper look into the underlying mathematics, please reference [5]."]},{"cell_type":"code","metadata":{"id":"cIl2EHv26kLS"},"source":["r_sq = model.score([REPLACE ME AND MY BRACKETS], [REPLACE ME AND MY BRACKETS]) # this line helps show us how good our model is\n","print('coefficient of determination:', r_sq) # print the model score"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"d2MuzvcC6wCp"},"source":["From using this `LinearRegression()` class and its method `.fit(x, y)` we have output model which has different methods. Above, we looked at the method `.score(x, y)` which output the $r^2$ value for this set of input output pairs with this trained model.\n","\n","Below, we extract the learned y-intercept, and slope. We can use these values to then predict unknown input/output pairs in the future!"]},{"cell_type":"code","metadata":{"id":"UhYsbfsG65mE"},"source":["print('intercept:', model.[REPLACE ME AND MY BRACKETS]) # print the y-intercept\n","print('slope:', model.[REPLACE ME AND MY BRACKETS]) # print the slope"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4Orfm43567on"},"source":["### Step 5: Make Predictions\n","\n","After fitting the model, we would now like to visualize the results and then use the results to predict future outputs of new inputs."]},{"cell_type":"code","metadata":{"id":"uYuGVVwX6-bb"},"source":["plt.style.use('ggplot') # we use the style 'ggplot' because it is pretty\n","\n","plt.plot([REPLACE ME AND MY BRACKETS], [REPLACE ME AND MY BRACKETS], 'ko') # plot the original data in black ('k') circles ('o')\n","\n","x_test = np.array([10, 12, 37, 10.4, 5, 57]).reshape((-1, 1)) # synthetic x-data\n","print(x_test)\n","\n","y_pred_package = model.[REPLACE ME AND MY BRACKETS](x_test) # use a predefined method to predict outputs from a set of inputs\n","print('predicted response from package:', y_pred_package, sep='\\n') # print prediction results\n","plt.plot(x_test, [REPLACE ME AND MY BRACKETS], 'r') # plot the learned relationship in a red ('r') line (default)\n","\n","# it is crucial to always fully annotate your plots\n","plt.xlabel('X-Data (Unit)')\n","plt.ylabel('Y-Data (Unit)')\n","plt.title('Linear Regression')    \n","plt.legend(['training data', 'learned model'])   \n","plt.show() # show the plot generated for each case"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"js4iiSKd7AW9"},"source":["Above, we used a built-in function `.predict` to generate our estimated outputs from our learned relationship or the results from the model fit. To show that we understand what we just completed, however, we can verify. that using the parameters we solved for by replacing them in the model equation. If this gives us the same result, then we know that we have a deep understanding of the model!\n","\n","We know that $y = mx + b$ is the model that we are trying to fit. We solved for the y-intercept or $b$, along with the slope or $m$. Using our learned paramters, we can generate our predictions by coding the equation by hand, as shown below."]},{"cell_type":"code","metadata":{"id":"axpZ1_jl7CLh"},"source":["plt.plot([REPLACE ME AND MY BRACKETS], [REPLACE ME AND MY BRACKETS], 'ko') # plot the original data in black ('k') circles ('o')\n","\n","x_test = np.array([10, 12, 37, 10.4, 5, 57]).reshape((-1, 1)) # synthetic x-data\n","print(x_test)\n","\n","y_pred_user = ([REPLACE ME AND MY BRACKETS] * x_test) + [REPLACE ME AND MY BRACKETS] # generate the same predicted relationship using parameters\n","print('predicted response from user:', y_pred_package, sep='\\n') # print prediction\n","plt.plot(x_test, y_pred_user, 'r') # plot the learned relationship in a red ('r') line (default)\n","\n","# it is crucial to always fully annotate your plots\n","plt.xlabel('X-Data (Unit)')\n","plt.ylabel('Y-Data (Unit)')\n","plt.title('Linear Regression')    \n","plt.legend(['training data', 'learned model'])   \n","plt.show() # show the plot generated for each case"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Z-efmFIXU_fE"},"source":["## Tutorial 2: K-means \n","\n","In this second tutorial, we will explore a method of **unsupervised learning** known as K-means clustering.\n","\n","K-means clustering is used to classify data. The ultimate goal is to partition the given data into a few groups or clusters. The way that this algorithm learns the appropriate clusters is by placing centroids randomly on the graph, calculating the distance of each point to its closest centroid, recomputing the centroid as the mean of all points and then iterating. The number of centroids is a user-prescribed parameter.\n","\n","To get some extra help with the underlying training procedure used in K-means, please watch [4], as it is a great explanation.\n","\n","Similar to the procedure in LSLR, we will follow a few main steps:\n","\n","1. Import and download all necessary packages and classes\n","2. Identify and prepare the data you desire to learn about\n","3. Create the model and fit it with the existing data.\n","4. Check the results of model fitting to know whether the model is satisfactory.\n","5. Apply the model for predictions.\n","\n","We will use an MEG data set for application of this method.\n","\n","### Step 1: Import Packages and Classes\n","\n","Once again, we have to import all of the packages necessary to implement our model. In this case, we are importing two packages unique to the packages used before. The first is the `KMeans()`"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":false,"id":"RbFoiXGXU_fF"},"outputs":[],"source":["import numpy as np # a package with many useful mathmatical functions\n","import matplotlib.pyplot as plt # a package necessary for plotting\n","from sklearn.cluster import KMeans # a package which contains a useful KMeans() class\n","import scipy.io as sio # a package which allows us to upload .mat files or files from MATLAB\n","import os"]},{"cell_type":"markdown","metadata":{"id":"ux2IRXQ9U_fF"},"source":["### Step 2: Provide Data\n","\n","In this case, we are using real data from a group of volunteers. Because of this, it is pivotal for us to have a deep understanding of the experiment being performed and what aspect of the experiment the data is encompasing.\n","\n","In this tutorial, we are looking at data from a **magnetoencephalography (MEG)** experiment. This MEG using $306$ sensors which take readings on the magnetic fields being produced by synchronous neural activity in the brain. If you are looking to get a better understanding of MEG experimental data refer to the video series in [6].\n","\n","In the specific MEG dataset we are using, each subject viewed $25$ different scenes from five different scene categories (forests, highways, mountains, beaches, cities). We have the MEG responses of each patient for different trials of viewing different scenes within these scene categories.\n","\n","To summarize, there are:\n","- $306$ MEG sensors\n","- $25$ different scenes\n","- $5$ scene categories\n","- $1$ scene from $1$ scene category per trial\n","- $125$ trials\n","- $1$ value for each sensor's activity per each trial\n","\n","**We are using K-Means Clustering for this MEG experimental data to see if we can identify possible clusters for each scene category, such that we could predict the categories of MEG data from new trials. If we can do this, this means that the brain is encoding the visual stimuli in a way that is decodable by MEG signals.**\n","\n","Now let's use a function to see each of the variables within the data files that contain the experimental data."]},{"cell_type":"code","source":["# Remove any existing folder\n","!rm -rf Introductory-Machine-Learning-Tutorial/"],"metadata":{"id":"5BS_L5Cx2GnU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Upload Repository to Google Directory\n","%%bash\n","git clone https://github.com/teb5240/Introductory-Machine-Learning-Tutorial.git"],"metadata":{"id":"GvTLDjVZU-Tz"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MAuaYaXlU_fF"},"outputs":[],"source":["# Navigate to Patient Data\n","data_path = '/content/Introductory-Machine-Learning-Tutorial'\n","os.chdir(data_path)\n","\n","# Look at Data\n","sio.whosmat('MEG_decoding_data_final.mat') # this gives us a better look at what is contained in the .mat file"]},{"cell_type":"markdown","metadata":{"id":"GAJh2Fe5U_fF"},"source":["`MEG_data` is a $125\\times306$ matrix with $125$ rows and $306$ columns. We know that the $306$ columns most likely corresponds to the number of sensors for the MEG. This means that each column is data from a different sensor. The rows correspond to the data from $125$ different trials. In each trial, the subject viewed one of the 25 images. The units of this value is in Tesla (T).\n","\n","`stim_ID` is a $1\\times125$ matrix or a $125$ column vector containing a value between 1 and 25 indicating which image the subject is viewing for that trial. This makes sense, because in the last matrix, there were 125 rows corresponding to the different trials. So, in this case, we have a vector indicating which stimulus is being shown for each row! This vector is unitless.\n","\n","`cat_ID` is another $125$ column vector containing which of the 5 stimulus or scene categories each trial was showing. So, instead of an integer between 1 and 25, this vector contains values between 1 and 5 where the integer:category pairs are 'beach':1, 'building':2, 'forest'3, 'highway':4, 'mountain':5. This vector is also unitless."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HRgtBfiKU_fF"},"outputs":[],"source":["mat_1 = sio.loadmat('MEG_decoding_data_final.mat') # loads a .mat file\n","\n","MEG = mat_1['MEG_data'] # isolates the MEG_data array from the dictionary\n","print('MEG Shape: ' + str(MEG.shape)) # prints the dimensions of the data within the variable\n","\n","stim = mat_1['stim_ID'] # isolates the stim_ID array from the dictionary\n","print('Stim Shape: ' + str(stim.shape)) # prints the dimensions of the data within the variable\n","\n","cat = mat_1['cat_ID']\n","print([REPLACE ME AND MY BRACKETS]) # prints the dimensions of the data within the variable"]},{"cell_type":"markdown","metadata":{"id":"nsH-9sr1U_fG"},"source":["In the above code, we have loaded in the two data files, and then isolated the `MEG_data`,  `stim_ID`, and `cat_ID` matrices. Ideally, we would use many of the MEG sensors for our feature set. However, for this tutorial, we want to isolate just two sensors such that we can visualize the data in a two-dimensional plot (the xy plots everyone is used to looking at). The next code section does just that."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DquRv7F-U_fG"},"outputs":[],"source":["current_data = MEG[:,[[REPLACE ME AND MY BRACKETS], [REPLACE ME AND MY BRACKETS]]] # isolates only channels 200 and 233 from the data\n","print(current_data.shape) # prints the shape of the data within the variable"]},{"cell_type":"markdown","metadata":{"id":"mHXZVCYsU_fG"},"source":["We want each of the $125$ trials, so we use the `:` syntax in the row parameter. We only want two sensors, so we input a vector with two sensor numbers. When we look at the shape, it shows that we have isolated two columns of $125$ trials, which is what we wanted!"]},{"cell_type":"markdown","metadata":{"id":"BsmuzJCcU_fG"},"source":["### Step 3: Create and Fit the Model\n","\n","Now that we have isolated our data, we will fit our model! For K-Means Clustering, we choose the number of clusters before fitting our model. In this case, we choose the number of clusters to fit to be 5, as there are 5 scene categories."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dgsEdCjMU_fG"},"outputs":[],"source":["Kmean_MEG = KMeans(n_clusters = [REPLACE ME AND MY BRACKETS]) # generates an instance of the KMeans() class\n","Kmean_MEG.fit([REPLACE ME AND MY BRACKETS]) # fits the model\n","\n","# What could you re-write the above two lines to be to generate the same results?\n","Kmean_MEG = KMeans([REPLACE ME AND MY BRACKETS]).fit([REPLACE ME AND MY BRACKETS]) # generates a class instance and fits the model"]},{"cell_type":"markdown","metadata":{"id":"U5HvgnIjU_fG"},"source":["### Step 4: Get Results\n","\n","We have fit our model above, and similar to the steps taking in the first tutorial, we want to look at the performance of our model. First, we will look at the placement of the centers. We do this with the `cluster_centers_` function below."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VCwlxCChU_fG"},"outputs":[],"source":["Kmean_MEG.cluster_centers_"]},{"cell_type":"markdown","metadata":{"id":"CjbSfzXKU_fG"},"source":["Next, we would like to visualize the learned centroids from the K-Means Clustering training procedure we just ran. To do this, we simply plot the original data where the x-axis is the MEG data corresponding to one sensor, and the y-axis is the MEG data of our other sensor. We do this below."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sp7jlI2uU_fG"},"outputs":[],"source":["plt.style.use('ggplot') # we use the style 'ggplot' because it is pretty\n","\n","plt.plot(current_data[[REPLACE ME AND MY BRACKETS],[REPLACE ME AND MY BRACKETS]], current_data[[REPLACE ME AND MY BRACKETS],[REPLACE ME AND MY BRACKETS]], 'ok') # plot the isolated data from out MEG file\n","\n","# this for loop iterates through some nice colors\n","for i, color in enumerate(plt.rcParams['axes.prop_cycle']):\n","    if i == 5: break # we only have 5 cluster centroids we want to plot\n","    plt.scatter(Kmean_MEG.cluster_centers_[i,0], Kmean_MEG.cluster_centers_[i,1], linewidth = 20, marker = '.', color = color['color']) # plot the centroids\n","\n","# it is crucial to always fully annotate your plots\n","plt.xlabel('MEG Data from Sensor 200 (T)')\n","plt.ylabel('MEG Data from Sensor 233 (T)')\n","plt.title('Learned K-Means Centroids')\n","plt.legend(['original data', 'beach', 'building', 'forest', 'highway', 'mountain'])\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"0ARtgMoTU_fH"},"source":["Now, we can look at which points were classified with each cluster by color coding the labeled points. First, we need the learned labels. We isolate these using the function `.labels` from the `KMeans()` class."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F5lZH86uU_fH"},"outputs":[],"source":["lab = Kmean_MEG.labels_ # gives us an array of class labels\n","print('Lab Size: ' + str(lab.size)) # prints the size of the variable"]},{"cell_type":"markdown","metadata":{"id":"MM_RwUDDU_fH"},"source":["This `lab` variable is a $125$ length row vector containing the learned cluster labels from the K-Means Clustering training procedure. \n","\n","If we print the values of this vector, we see that it contains values between $0$ and $4$. This is slightly unexpected, as we intuitively think the vector should be values between $1$ and $5$. Because we are working in Python, our initial index is always $0$ rather than $1$. This means that our first category is labeled as $0$ rather than $1$, as we originally had. This is crucial to keep in mind when coding. Depending on the language you are using, this inital index may change.\n","\n","With that in mind, let's plot!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AQKAMLUrU_fH"},"outputs":[],"source":["# once again, we are iterating through some nice colors\n","for i, color in enumerate(plt.rcParams['axes.prop_cycle']):\n","    if i == 5: break # we only have 5 clusters we want to plot\n","    idx_tmp = np.nonzero(lab == i) # for each index, isolate the indices where the label s equal to that index\n","    pt_tmp = current_data[idx_tmp] # isolate the data associated with that index\n","    plt.plot(pt_tmp[:,0], pt_tmp[:,1], linewidth = 0, marker = 'o', color = color['color']) # plot that isolated data\n","    plt.scatter(Kmean_MEG.cluster_centers_[i,0], Kmean_MEG.cluster_centers_[i,1], linewidth = 20, marker = '.', color = color['color']) # plot centroids\n","    \n","# it is crucial to always fully annotate your plots\n","plt.xlabel('MEG Data from Sensor 200 (T)')\n","plt.ylabel('MEG Data from Sensor 233 (T)')\n","plt.title('Centroids with Labeled MEG Data')    \n","plt.legend(['beach', 'building', 'forest', 'highway', 'mountain'])\n","plt.show() # SHOW YOUR COLORS WOOOO"]},{"cell_type":"markdown","metadata":{"id":"T1Hd1BmoU_fH"},"source":["Awesome! This looks great. We seem to be clustering the data in a reasonable fashion.\n","\n","Below, we do this same procedure five times, and plot the results. If you looks closly at the figures, you see that the colors are changing with each plot. But, not only are the colors changing; the actual cluster designations or labels are changing themselves. Each time we run the K-Means Clustering algorithm, we get a new set of centroids, and a new set of labels.\n","\n","This variability occurs because of the design of the training procedure. We begin with randomly placed centroids. This random placement means that the process leading up to the final learned clusters is almost always different and, subsequently, the final cluster positions may vary slightly with each instance of K-Means. Look for this in the plots below!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nEUTCGGNU_fH"},"outputs":[],"source":["# Let's now run K-means on the same data five times and look at what changes\n","for i in range(0,5):\n","\n","    Kmean_MEG = KMeans(n_clusters = 5).fit(current_data) # generate a KMeans() class instance and fit\n","\n","    lab = Kmean_MEG.labels_ # labels for each data point \n","\n","    for i, color in enumerate(plt.rcParams['axes.prop_cycle']):\n","        if i == 5: break # break after 5 cases are plotted\n","        idx_tmp = np.nonzero(lab == i) # for each index, isolate the indices where the label s equal to that index\n","        pt_tmp = current_data[idx_tmp] # isolate the data associated with that index\n","        plt.plot(pt_tmp[:,0], pt_tmp[:,1], linewidth = 0, marker = 'o', color = color['color'])\n","        plt.scatter(Kmean_MEG.cluster_centers_[i,0], Kmean_MEG.cluster_centers_[i,1], linewidth = 20, marker = '.', color = color['color'])\n","            \n","    # it is crucial to always fully annotate your plots\n","    plt.xlabel('MEG Data from Sensor 200 (T)')\n","    plt.ylabel('MEG Data from Sensor 233 (T)')\n","    plt.title('Centroids with Labeled MEG Data')    \n","    plt.legend(['beach', 'building', 'forest', 'highway', 'mountain'])   \n","    plt.show() # show the plot generated for each case"]},{"cell_type":"markdown","metadata":{"id":"TpDSReFxU_fH"},"source":["Now that we have shown that different instances of K-Means may yield different clustering designations, we should think about how to choose the best K-Means fit. \n","\n","Let us imagine that we run K-Means Clustering of out sensor 200 and sensor 233 MEG data and repeat the clustering ten times. We can look at the **average centroid-to-data-point distance**. If we had to choose the K-means analysis with the best clustering to move forward with, we should choose the smallest **average centroid-to-data-point distance**."]},{"cell_type":"markdown","metadata":{"id":"bD_la2yKU_fH"},"source":["### Step 5: Make Predictions\n","\n","We do not explicitly explore step 5 in this tutorial, but it is useful to consider how we might use the K-means Clustering algorithm to generate predictions about new data. In this case, we could isolate the learned centroids, plot our new data and assess each point to determine the closest centroid."]},{"cell_type":"markdown","source":["## Challenge: SVM\n","\n","**Goal: Attempt to train a support vector machine to classify one scene category versus another. For example, train the classifier to distinguish scene category 1 from scene category 2. Use the SVM package from sklearn: `from sklearn import svm`**\n","\n","To remind you, we have the description of the data below.\n","\n","In the specific MEG dataset we are using, each subject viewed $25$ different scenes from five different scene categories (forests, highways, mountains, beaches, cities). We have the MEG responses of each patient for different trials of viewing different scenes within these scene categories.\n","\n","To summarize, there are:\n","- $306$ MEG sensors\n","- $25$ different scenes\n","- $5$ scene categories\n","- $1$ scene from $1$ scene category per trial\n","- $125$ trials\n","- $1$ value for each sensor's activity per each trial\n","\n","`MEG_data` is a $125\\times306$ matrix with $125$ rows and $306$ columns. We know that the $306$ columns most likely corresponds to the number of sensors for the MEG. This means that each column is data from a different sensor. The rows correspond to the data from $125$ different trials. In each trial, the subject viewed one of the 25 images. The units of this value is in Tesla (T).\n","\n","`stim_ID` is a $1\\times125$ matrix or a $125$ column vector containing a value between 1 and 25 indicating which image the subject is viewing for that trial. This makes sense, because in the last matrix, there were 125 rows corresponding to the different trials. So, in this case, we have a vector indicating which stimulus is being shown for each row! This vector is unitless.\n","\n","`cat_ID` is another $125$ column vector containing which of the 5 stimulus or scene categories each trial was showing. So, instead of an integer between 1 and 25, this vector contains values between 1 and 5 where the integer:category pairs are 'beach':1, 'building':2, 'forest'3, 'highway':4, 'mountain':5. This vector is also unitless."],"metadata":{"id":"yLY8t00nfsFZ"}},{"cell_type":"markdown","source":["**You did it! Congratulations!**"],"metadata":{"id":"HZqXyiCbmE5c"}},{"cell_type":"markdown","metadata":{"id":"SPKoYTYeknok"},"source":["# Sources\n","1. Murphy, Kevin P.. Machine Learning: A Probabilistic Perspective, MIT Press, 2012. ProQuest Ebook Central, http://ebookcentral.proquest.com/lib/pensu/detail.action?docID=3339490.\n","2. https://realpython.com/linear-regression-in-python/#linear-regression\n","https://github.com/scikit-learn/scikit-learn/blob/7813f7efb/sklearn/linear_model/base.py#L367"]}]}